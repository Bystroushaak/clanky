<p>Povídání o GPT a LLM (Large Language Models), návod jak si rozjet menší modely doma a pár ukázek jak používám GPT4.</p>
    <!--break-->

<p>Původně publikováno na <a href="https://blog.rfox.eu/cz/Programovani/Jak_na_vlastni_LLM_GPT.html">blogu autora</a>.</p>

<hr />

    <blockquote>
        Představte si, že jste stroj.
        <p>Jasně, já vím. Ale představte si, že jste <em>jiný druh stroje</em>, postavený z kovu a plastu a navržený ne slepým, náhodným přirozeným výběrem, ale inženýry a astrofyziky, kteří mají oči pevně upřené na konkrétní cíle. Představte si, že vaším účelem není rozmnožovat se, nebo dokonce přežít, ale shromažďovat informace.</p><p>— <a href="https://www.databazeknih.cz/knihy/slepozrakost-77865">Slepozrakost</a>, Peter Watts</p></blockquote><hr /><p>Celebrity technického světa se <a href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/">bouří</a>, že by to chtělo pozastavit vývoj na poli velkých jazykových modelů na alespoň 6 měsíců.</p><ul><li><a href="https://www.seznamzpravy.cz/clanek/tech-technologie-zastavte-vyvoj-umele-inteligence-volaji-odbornici-a-musk-lidstvo-to-nestiha-228605">Naléhavá výzva hvězd IT: Zastavte vývoj umělé inteligence, jde moc rychle</a></li></ul><ul><li><a href="https://denikn.cz/minuta/1113210/">Musk podepsal otevřený dopis pro zastavení vývoje umělé inteligence</a></li></ul><ul><li><a href="https://www.idnes.cz/technet/software/musk-ai-umela-inteligence.A230329_095318_software_alv">Musk a odborníci žádají pozastavení vývoje umělé inteligence, bojí se rizik</a></li></ul><p>Jako správní chatičtí neutrálové si tedy rozjedem vlastní AI doma. Ale s předmluvou a nějakým tím kontextem, ať vlastně víme co děláme.</p><h1>Jak chápat GPT / LLM</h1><p>Setkávám se s jedním zásadním nepochopením, které často mají lidi co trochu tuší jak to funguje;</p><blockquote>
        Je to jen doplňovač dalšího slova (tokenu).
      </blockquote><p>Což jako jo, ale technicky vzato když píšete na klávesnici, tak jste taky jen doplňovače dalšího slova.</p><p><a href="/images/screenshots/2/1/266512-jak-na-vlastni-llm-gpt--1257338851884427958.png"><img width=600 src="/images/screenshots/2/1/266512-jak-na-vlastni-llm-gpt--8462072709914681360.png" /></a><br /><sub><em>(Zdroj:</em><em><a href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/">What Is ChatGPT Doing … and Why Does It Work?</a></em><em>)</em></sub></p><p>Jak vysvětluje třeba Ilya Sutskever (jeden z tvůrců):</p><p><a href="https://www.youtube.com/watch?v=Yf1o0TQzry8?start=394"><img width=600 src="/images/screenshots/2/1/266512-jak-na-vlastni-llm-gpt--3096530850425322638.jpg" /></a></p><p>K tomu aby neuronová síť mohla predikovat další token nad gigantickým datasetem, jímž byla trénována, si musí vytvořit bohaté vnitřní reprezentace. Ty zahrnují nejen všechny možné lidské jazyky, ale i znalosti, vztahy a vzory.</p><p>Tohle se někdy označuje jako <em>“komprese”</em>, protože síť je, způsobem jakým je trénována na velkém množství dat, nucena vytvořit tyto reprezentace nad omezeným setem vah a neuronů. Mohla by teoreticky ukládat miliardy ukázek stylem čínského pokoje, kde si prostě uloží <em>“otázka”</em> - <em>“odpověď”</em> (počátek doplňovaného textu - následující token). Ale tím jak je nucena operovat s omezeným množstvím dostupné vnitřní paměti jí nezbývá nic jiného, než pochopit stále abstraktnější vzory. Nějakým způsobem si to vnitřně reprezentovat jako <em>“znalosti”</em> a <em>“chápání”</em>.</p><p><a href="/images/screenshots/2/1/266512-jak-na-vlastni-llm-gpt--292339293632205263.png"><img width=600 src="/images/screenshots/2/1/266512-jak-na-vlastni-llm-gpt--8902434558212371416.png" /></a><br /><sub><em>(PlantUML vygeneroval GPT4, obrázek vpravo</em><em><strong>DALL·E 2</strong></em><em>)</em></sub></p><p>Tohle rozpoznávání vzorů se děje jak na úrovni jazyka (syntaxe a gramatika), tak na úrovni <em>word embeddings</em> (význam slov, jak spolu souvisí), tak na mnohem abstraktnější úrovni (jak fungují věci o kterých je řeč a jak spolu souvisí).</p><p>Nefunguje to tedy tak, že to doplňuje konverzace doslovně na základě toho co už to někde vidělo. Proto jsou tyto modely schopné například překládat mezi jazyky líp než všechno co dosud existovalo - protože opravdu rozumí kontextu toho o čem je řeč.</p><p>Pokud by model jen doplňoval na základě toho co už někde viděl, tak by překládat schopný nebyl, pokud by předtím danou větu, nebo ideálně celý odstavec už někde neviděl.</p><p>Jak <a href="https://www.youtube.com/watch?v=goOa0biX6Tc">říká</a> Ilya Sutskever, představte si, že při trénování doplňuje třeba text detektivky, kde vrah je oznámen až na poslední stránce knihy plné různého vyšetřování. Aby byla síť schopna korektně doplnit jméno vraha když ho v textu detektiv vysloví, musí <em>chápat</em> všechny možné souvislosti, celé vyšetřování, různé důkazy a tak podobně.</p><h2>Simulátory</h2><p>Předtím než přišel chat mode byly výsledky, například u GPT3, poměrně nevalné. Jasně, něco to dělalo, ale člověk se z toho po počátečním překvapení úplně neposadil na zadek, a dost často se to vydalo úplně jiným směrem, než bych chtěl. Celkově to působilo dost omezeně.</p><p>Jak už pozorovalo mnoho lidí, dramaticky záleželo na promptu který byl AI zadán. Někdy neuměla vysvětlit nebo udělat nic. Jindy to zvládla, když se jí řeklo ať předstírá že je Sherlock Holmes.</p><p>Za tohle může v podstatě způsob jakým byla trénována a že je to doplňovač textu. Má prostě tendence doplňovat. K tomu aby doplňovala chytře a užitečně je v podstatě třeba nastavit kontext tak, že <em>doplňuje</em> příběh o chytré a užitečné postavě - třeba Sherlocku Holmesovi. Tak jak by to bylo v trénovacích datech. Chytré chování v příbězích o chytrých lidech. Pokud člověk nechal doplnit něco bez patřičného kontextu, tak výsledky byly dost náhodné.</p><p><a href="https://astralcodexten.substack.com/p/janus-simulators">Janus' Simulators</a> je krásný blog na tohle téma, se spoustou ukázek.</p><h2>Chat</h2><p>S tím přichází překvapivě vysoká užitečnost chatu. Chat není nějaká radikálně nová funkcionalita, ale jen způsob jakým je používáno auto-doplňování. Například v <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> najdeme v podsložce <code>prompts/</code> soubor <code>chat-with-bob.txt</code>. Ten má následující obsah:</p><pre><code>Transcript of a dialog, where the User interacts with an Assistant named Bob. Bob is helpful, kind, honest, good at writing, and never fails to answer the User's requests immediately and with precision.

User: Hello, Bob.
Bob: Hello. How may I help you today?
User: Please tell me the largest city in Europe.
Bob: Sure. The largest city in Europe is Moscow, the capital of Russia.
User:</code></pre><p>Celý <em>chat mode</em> funguje úplně triviálně - prvně předhodí síti k doplňování přepis rozhovoru s AI asistentem, kde je na začátku nějaký <em>prompt</em>, potom následuje ukázka <em>formátu</em> (otázka, odpověď). Jakmile program narazí ve výstupu na pattern <code>User:</code>, použije se jednoduchý pattern matching:</p><pre><code>-r PROMPT, --reverse-prompt PROMPT
     run in interactive mode and poll user input upon seeing PROMPT (can be
     specified more than once for multiple prompts).</code></pre><p>Pokud najde string v tomhle parametru, načte trochu dat od uživatele, přidá je k původnímu dokumentu a pokračuje v doplňování. Tím vzniká celá iluze chatu, přestože model pořád jen dál doplňuje “přepis” rozhovoru člověka s umělou inteligencí.</p><p>Když se podíváte na ten <em>prompt</em>, tak tam je uveden kontext simulátoru - přepis konverzace s užitečným nápomocným asistentem, který má <em>formát</em> střídajících se otázek a odpovědí. Model se tedy chová užitečně a nápomocně, protože doplňuje příběh o tom jak by to vypadalo, kdyby se choval užitečně a nápomocně.</p><p>Z toho taky plyne že když se bavíte s chatem na openAI, nemáte přístup k tomu jak vypadá ten kontext. Ovšem pokud jdete do <a href="https://platform.openai.com/playground">playgroundu</a>, můžete si ten <em>prompt</em> do jisté míry nastavit (píšu <em>do jisté</em><em>míry</em>, protože OpenAI k tomu imho přidává vlastní <em>prompt</em>):</p><p><img src="/images/screenshots/2/1/266512-jak-na-vlastni-llm-gpt--6755333531227410992.png" width="900" /></p><p>Zde je ukázka reakce s jiným <em>promptem</em>:</p><p><img src="/images/screenshots/2/1/266512-jak-na-vlastni-llm-gpt--1846014972060060366.png" width="900" /></p><p><em>Prompty</em> taky můžou uvádět podstatně složitější <em>formát</em>, například jde simulovat jakési hlubší přemýšlení nad otázkami, viz <a href="https://github.com/ggerganov/llama.cpp/blob/master/prompts/reason-act.txt">llama.cpp/prompts/reason-act.txt</a>:</p><pre><code>You run in a loop of Thought, Action, Observation.
At the end of the loop either Answer or restate your Thought and Action.
Use Thought to describe your thoughts about the question you have been asked.
Use Action to run one of these actions available to you:
- calculate[python math expression]
Observation will be the result of running those actions


Question: What is 4 * 7 / 3?
Thought: Do I need to use an action? Yes, I use calculate to do math
Action: calculate[4 * 7 / 3]
Observation: 9.3333333333
Thought: Do I need to use an action? No, have the result
Answer: The calculate tool says it is 9.3333333333
Question: What is capital of france?
Thought: Do I need to use an action? No, I know the answer
Answer: Paris is the capital of France
Question:</code></pre><p>Tedy model se jen nesnaží dát odpověď, ale prvně se <em>Zamyslí</em>, potom naplánuje <em>Akci,</em> následovanou <em>Pozorováním</em>, po kterém se znovu <em>Zamyslí</em> a nakonec poskytne <em>Odpověď</em>. Tímhle je možné obejít některé nedostatky modelu, jako například krátkodobou paměť, nebo problémy s dlouhodobým plánováním.</p><h2>Human alignment & shoggoth</h2><p>S <em>chat modem</em> se ukázalo, že existující LLM můžou fungovat jako užitečná AI, akorát jsou úplně cizí našemu očekávání a často nedělají co po nich chceme.</p><p>Zajímavý vývoj posledních asi půl roku je, že se dají poměrně rychle ohnout pomocí <a href="https://huggingface.co/blog/rlhf">RLHF</a> (Reinforcement Learning from Human Feedback), tedy něco jako <em>učení z lidské zpětné vazby</em>. To spočívá v tom, že původní fungující model se rozšíří nějakou další vrstvou která udává <em>vhodnost</em> odpovědi, a pak se doučí na různých ukázkách konverzací <em>přijatelné chování</em>. Model se neučí nová fakta, nebo novým způsobem uvažovat o světě, ale v podstatě co od něj chceme, co je pro lidi relevantní a co není. <em>Human alignment</em> (<em>příklon k lidskosti?</em>).</p><p>Tak vznikl meme Shoggoth, příšery s lidskou maskou, protože na pozadí je to pořád něco úplně cizího, simulátor kterému byla nasazena přívětivá maska:</p><p><a href="/images/screenshots/2/1/266512-jak-na-vlastni-llm-gpt--8647307315918901693.jpeg"><img width=400 src="/images/screenshots/2/1/266512-jak-na-vlastni-llm-gpt--2315368010019494110.jpg" /></a>
  
  <a href="/images/screenshots/2/1/266512-jak-na-vlastni-llm-gpt--2869854406078394666.jpeg"><img width=400 src="/images/screenshots/2/1/266512-jak-na-vlastni-llm-gpt--9084783075062241569.jpeg" /></a>
  
  <a href="/images/screenshots/2/1/266512-jak-na-vlastni-llm-gpt--5149310911213684303.jpg"><img width=400 src="/images/screenshots/2/1/266512-jak-na-vlastni-llm-gpt--4510852857279770782.jpg" /></a>
  
  <a href="/images/screenshots/2/1/266512-jak-na-vlastni-llm-gpt--1332350006368482244.png"><img width=400 src="/images/screenshots/2/1/266512-jak-na-vlastni-llm-gpt--7749220047154575833.png" /></a>
  
  <a href="/images/screenshots/2/1/266512-jak-na-vlastni-llm-gpt--8953491451105480308.jpeg"><img width=400 src="/images/screenshots/2/1/266512-jak-na-vlastni-llm-gpt--3389705137340020203.jpeg" /></a>
  
  <a href="/images/screenshots/2/1/266512-jak-na-vlastni-llm-gpt--3432976670240455937.jpg"><img width=400 src="/images/screenshots/2/1/266512-jak-na-vlastni-llm-gpt--2758687499215042836.jpg" /></a>
  
  <a href="/images/screenshots/2/1/266512-jak-na-vlastni-llm-gpt--2763059354832979330.jpg"><img width=400 src="/images/screenshots/2/1/266512-jak-na-vlastni-llm-gpt--2303085439314207798.jpg" /></a></p>
  
  <p>Ty obrázky jsou cute, ale jak už poznamenal někdo na twitteru, jsou principiálně špatná analogie. Správné zobrazení by bylo místo mnoha očí mít mnoho masek, protože Shoggoth sám o sobě v podstatě není.</p><p><img src="/images/screenshots/2/1/266512-jak-na-vlastni-llm-gpt--5860205795968310226.jpg" width=600 /></p><p>Je to něco jako hlas všech textů lidstva s maskou všech postav všech příběhů. Tomu někdo navrch nasprejoval smajlíka ve tvaru chatbota, se kterým si myslíte, že si povídáte.</p><p>To nemyslím tak jako <em>“berte informace s rezervou”</em>, nebo <em>“nespoléhejte na ně”</em>, ale spíš jako že nátura Shoggotha je Shoggoth. Shoggoth musí simulovat, aby doplňoval text. Když nechápe, tak ve většině případů vypadl z role a neví co má předstírat. Musíte mu ten kontext uvést. Ne kontext rozhovoru, ale toho co má být, jako co má vést rozhovor. Meh.</p><h2>Malé modely, proč a jak fungují</h2><p>Článek <a href="https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications">chinchilla's wild implications</a> ukazuje na proč není počet vah (parametrů) úplně směrodatná metrika, která by měla být cílem, a že víc dat vyhrává nad větším množstvím <em>parametrů</em> modelu.</p><p>Poněkud překvapivě se ukázalo, že když člověk vezme tyhle <em>human alignment</em> data a nacpe je do výrazně menších modelů, tak se jde v různých benchmarcích dostat někam na úroveň lehce pod GPT3. GPT3 je 175B, GPT4 je údajně 6x větší, ale čísla jsem nenašel, někdo tvrdí že má 10x víc parametrů. “B” v popisu je tam pro anglický <em>billion</em>, česky <em>miliarda</em> (parametrů).</p><p>Story za tím;</p><p>Facebook (Meta AI, ehm) vytvořil set relativně malých <em><a href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai/">llama</a></em> modelů 7B, 13B, 33B, a 65B, které natrénovali tak nějak standardním způsobem. Pak to všechno +- zveřejnili. Původně asi úplně, zpětně se v shitstormu kolem veřejnosti diskutující ohledně zneužití GPT4 rozhodli ho dávat jen po vyplnění formuláře dalším <em>výzkumníkům (.edu mail je velké plus)</em>. Samozřejmě se stalo očividné a modely jsou šířeny všude možně (torrent, mrk mrk).</p><p>Lidi ze Standfordu vzali ten nejmenší <em>llama</em> 7B model a použili na něm <a href="https://arxiv.org/pdf/2212.10560.pdf">self-instruct</a> fine tuning, čímž stvořili <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">Alpacu</a>.</p>

<p><a href="https://www.youtube.com/watch?v=xslW5sQOkC8"><img width=600 src="/images/screenshots/2/1/266512-jak-na-vlastni-llm-gpt--8433857170403871696.jpg" /></a></p>

<p>Na tomhle je zajímavé, že narozdíl od původního RLHF, které probíhalo formou tisíce hodnocených konverzací s uživateli ve stylu <em>“uživatel si s modelem povídá, pak vybere jestli dobrý nebo špatný, model se pak trénuje aby dělal víc dobrý a míň špatný”</em> k tomu použili GPT3.5. Tedy <em>“jeden model trénuje druhý model na dobrý a špatný”</em>. Tím byl pronesen úvodní přípitek večírku <a href="https://cs.wikipedia.org/wiki/Technologick%C3%A1_singularita">Singularity</a> (bájný stav, kdy naše technologie začne vylepšovat sebe sama).</p><p><a href="/images/screenshots/2/1/266512-jak-na-vlastni-llm-gpt--4465788888856703294.png"><img src="/images/screenshots/2/1/266512-jak-na-vlastni-llm-gpt--610364940456695451.png" /></a></p><p>Překvapivě se ukázalo, že stačí asi 52 tisíc těhle ukázek, tedy v podstatě nic ve srovnání s množstvím ostatních trénovacích dat, a model se dostává v benchmarcích a automatizovaných testech o desítky procent blíž k úrovni GPT3.</p><p>Alpaca potom byla <a href="https://github.com/tatsu-lab/stanford_alpaca">uvolněna</a> super divným způsobem; protože původní model byl facebooku a ten ho přestal šířit, uvolnili v podstatě jen něco jako diff od <em>llamy</em>. Tedy k tomu aby se dal rozchodit bylo potřeba někde sehnat <em>llamu</em>. Všichni se plácali po zádech, jak jsou zodpovědní a brání šíření spamu a dezinformací. To jim vydrželo přibližně do druhého dne, než to někdo zkombinoval a hodil na net.</p><p><em>Podle mého současného názoru</em> tohle všechno znamená, že ty žádané a zajímavé schopnosti jsou i v menších modelech. Ta těžká část, která byla dosud řešena čím dál větším počtem parametrů spočívá do jisté míry čistě v tom aby ten model vyabstrahoval co po něm vlastně chcem. Což tam jde dohackovat mnohem levněji.</p><p>Co ovšem zavětřila asi tak půlka internetu nebyl ani tak samotný model, ale informace že:</p><blockquote><em>Alpaca behaves qualitatively similarly to OpenAI’s text-davinci-003, while being surprisingly small and easy/cheap to reproduce (&lt;600$).</em></blockquote><blockquote><em>while being surprisingly small and easy/cheap to reproduce (&lt;600$).</em></blockquote><blockquote><em>cheap to reproduce (&lt;600$).</em></blockquote><blockquote><em>600$</em></blockquote><p>Jen pro kontext, téhle úrovně funkcionality se u větších modelů se pohybuje v řádu milionů dolarů. Party začala. Během posledních pár týdnů proběhlo kvašení, jehož výsledkem je mimo jiné:</p><ol><li><a href="https://www.reddit.com/r/LocalLLaMA/">r/LocalLLaMA</a> subreddit věnovaný provozování, používání a trénování malých modelů
        </li>
        <li><a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> přepis python kódu do C++, takže běží podstatně rychleji i na CPU
        </li>
        <li><a href="https://www.reddit.com/r/Oobabooga/">r/Oobabooga</a> a <a href="https://github.com/oobabooga/text-generation-webui/">Text generation web UI</a> (user friendly web UI ve stylu <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui">stable-diffusion-webui</a>)
        </li>
        <li><a href="https://vicuna.lmsys.org/">Vicuna</a> (v době článku asi nejschopnější malý model)
        </li>
        <li><a href="https://github.com/nomic-ai/gpt4all">GPT4All</a> llama model natrénovaný na ~800k GPT3 konverzacích, s binárkami i scripty i modely a vším
        </li>
        <li><a href="https://github.com/tloen/alpaca-lora">Alpaca-LoRA</a> alternativa k alpace za použití <a href="https://arxiv.org/pdf/2106.09685.pdf">LoRA</a> (specifický způsob jak trénovat existující modely)
        </li>
        <li><a href="https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM">GPT-4-LLM</a> trénovací data pro <em>fine tuning</em> modelů
        </li>
        <li><a href="https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered">ShareGPT_Vicuna_unfiltered</a> trénovací data ze kterých bylo vyhozeno filtrování (sex, rasismus a tak podobně)
        </li></ol><p>a každý den něco dalšího.</p><p>Mimochodem, tohle je jeden z těch úžasných momentů <em>na rozhraní</em>. Když se něco mění. Taková ta chvíle co se zdá divoká, ale člověk na to zpětně vzpomíná. Něco jako nostalgické devadesátky, nebo začátek bitcoinu. Ta doba co srší potenciálem, věci nejsou jasně dané a všechno je zdánlivě možné. Chce se to chvíli zastavit a ocenit tenhle okamžik, protože typicky vám to dojde až zpětně. Otevírají se nové dimenze (<a href="../../Ostatni/Vytvareni_prostoru_oteviranim_dimenzi.html">Vytváření prostorů otevíráním dimenzí</a>), kolabují a tak. Fakt cool.</p><h2>Jak rozjet lokální model</h2><p>Tenhle článek jsem původně začal psát, protože jsem to doma zkoušel a bylo to docela složité. Během doby co jsem ho psal se všechno natolik zjednodušilo, že původní text totálně ztratil smysl.</p><p>Rozjedeme si teď Vicunu (nebo si <a href="https://www.reddit.com/r/LocalLLaMA/wiki/models/">tady</a> vyberte něco jiného).</p><p>Někam na NVMe, kde mámě aspoň sto giga místa si prvně naklonujeme <a href="https://github.com/oobabooga/text-generation-webui/">textgeneration-web-ui</a>:</p><pre><code>git clone https://github.com/oobabooga/text-generation-webui.git</code></pre><p>Přejdeme do složky <code>models/</code> a dáme klonovat Vicunu:</p><pre><code>git clone https://huggingface.co/eachadea/vicuna-13b</code></pre><p>Jedná se o pytorch model. Zatímco se klonuje, otevřeme si další terminál, a nainstalujeme si závislosti. Prvně minicondu (nebo anacondu):</p><pre><code>curl -sL "https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh" &gt; "Miniconda3.sh"
bash Miniconda3.sh</code></pre><p>Potom nějaké ty nutnosti pro buildování:</p><pre><code>sudo apt install build-essential</code></pre><p>Vyrobíme si nový <code>conda</code> environment:</p><pre><code>conda create -n textgen python=3.10.9
conda activate textgen</code></pre><p>V něm pak nainstalujeme závislosti:</p><pre><code>pip3 install torch torchvision torchaudio
pip install -r requirements.txt</code></pre><p>No a to je všechno. Počkáme až se model stáhne a pak to celé spustíme:</p><pre><code>python server.py --cpu</code></pre><p>Parametr <code>--cpu</code> je možné vynechat, pokud máte grafickou kartu s 24G VRAM.</p><pre><code>$ python server.py --cpu --chat

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/bystrousak/anaconda3/envs/textgen/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
Loading vicuna-13b...
Loading checkpoint shards: 100%|██████████████████| 3/3 [00:14&lt;00:00,  4.95s/it]
Loaded the model in 15.10 seconds.
Running on local URL:  http://127.0.0.1:7860

To create a public link, set `share=True` in `launch()`.</code></pre><p>Na portu <code>7860</code> naběhlo webové rozhraní:</p><p><a href="/images/screenshots/2/1/266512-jak-na-vlastni-llm-gpt--3611762760061296258.jpg"><img src="/images/screenshots/2/1/266512-jak-na-vlastni-llm-gpt--4270406405546796216.jpg" width="900" /></a></p><p>Slova se u mě objevují velmi pomalu, asi tak jedno za pět vteřin. Postupně se načítá po znacích, jak ho model generuje. Chtělo by to grafickou kartu s větší pamětí, do mojí 3090Ti se model nevleze.</p><h3>llama.cpp</h3><p>Naštěstí je tu ještě <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>, C++ přepis python kódu:</p><pre><code>git clone https://github.com/ggerganov/llama.cpp.git</code></pre><p>Pak jí zbuildíme příkazem <code>make</code>.</p><p>Menší problém je, že <code>llama.cpp</code> používá optimalizovaný formát ukládání dat, který musíme z původního pytorch tensoru (<code>.pth</code>) překonvertovat na <code>.ggml</code>. V <code>llama.cpp</code> jsou na to různé konverzní scripty, které ovšem bohužel nefungují:</p><pre><code>$ python3 convert-pth-to-ggml.py ../models/vicuna-13b 0
Traceback (most recent call last):
  File "/media/bystrousak/internal_nvm/llama.cpp/convert-pth-to-ggml.py", line 274, in &lt;module&gt;
    main()
  File "/media/bystrousak/internal_nvm/llama.cpp/convert-pth-to-ggml.py", line 239, in main
    hparams, tokenizer = load_hparams_and_tokenizer(dir_model)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/media/bystrousak/internal_nvm/llama.cpp/convert-pth-to-ggml.py", line 102, in load_hparams_and_tokenizer
    with open(fname_hparams, "r") as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '../models/vicuna-13b/params.json'</code></pre><p>Chybí nám <code>params.json</code>, které jsem sice někde našel, ale pak chybí zase něco jiného a pak zase něco dalšího. V <a href="https://github.com/ggerganov/llama.cpp/pull/545">#545</a> je nový script <a href="https://github.com/ggerganov/llama.cpp/blob/b9c372dc7b571120e50cb1085fa05e0b209eeef5/convert.py">convert.py</a>. Ten si stáhněte do <code>llama.cpp</code>.</p><p>Aktivujeme zase <code>textgen</code> condu z předchozí kapitoly.</p><pre><code>$ conda activate textgen</code></pre><p>Při spuštění si stěžuje že chybí soubor <code>added_tokens.json</code>:</p><pre><code>Exception: Vocab size mismatch (model has 32001, but ../models/vicuna-13b/tokenizer.model has 32000).  Most likely you are missing added_tokens.json (should be in ../models/vicuna-13b).</code></pre><p>Takže ho tam přidáme:</p><pre><code>{
    "&lt;unk&gt;": 32000
}</code></pre><p>Kde jsem to vzal? Poslední hodnota v <code>tokenizer_config.json</code> ve složce vicuny. Úplně random a netuším jestli je to správně (spíš ne, ale +- to funguje).</p><pre><code>(textgen) $ python3 convert.py ../models/vicuna-13b --outtype f32</code></pre><p>Což vybleje soubor <code>../models/vicuna-13b/ggml-model-f32.bin</code>, ten se dá potom už pustit přes <code>llama.cpp</code>. Alternativně je možné dát jako parametr <code>--outtype f16</code> pro menší velikost, aby se vám to vešlo do paměti (ekvivalent <em>kvantizace</em> z jiných projektů).</p><p>No a pak už to zbývá jen spustit a hrát si s tím:</p><pre><code>./main -m ../models/vicuna-13b/ggml-model-f32.bin --color --repeat_penalty 1.0 -i -t 15 -r "User:" -f prompts/chat-with-bob.txt</code></pre><p>U mě na počítači se text objevuje rychlostí asi jednoho slova za vteřinu.</p>

<h3>Nutno dodat</h3><p>Menší modely jsou samozřejmě méně schopné, ale nejsou úplně neschopné. Pokud pozorujete divné chování, je možné:</p>

<ol><li>Máte ne-ideálně nastavené různé vedlejší hodnoty modelu. Těch je spousta a pokud jste je nenastavovali, tak je někdo nastavil za vás, a dost často blbě. Typicky <code>temperature</code> je něco co chcete mít někde u <code>0.7</code>.
          <p><a href="/images/screenshots/2/1/266512-jak-na-vlastni-llm-gpt--7953160818814971645.png"><img width=900 src="/images/screenshots/2/1/266512-jak-na-vlastni-llm-gpt--3349493586392193724.png" /></a></p></li>

<li><em>Prompt</em>. Jak jsem vysvětloval, LLM jsou simulátory. Mějte pořád na paměti, že nemluvíte s člověkem, mluvíte se Shoggothem co simuluje AI asistenta. Chce to podle toho konstruovat kontext / prompt. Pokud tam žádný nenastavíte, tak je to Shoggoth a místy mluví neexistujícími jazyky, místy se opakuje a místy bleje binární duhu.</li>

<li><em>Quantization</em>. Jde narazit na modely, které mají v názvu něco ve stylu <code>4bit</code> a tak podobně. Úplně jsem to nestudoval, ale jestli to chápu správně, tak jsou to v podstatě modely kde se váhy neuronové sítě přepočítaly na 4bitová čísla, aby se to vešlo do paměti. Tedy to ztratilo přesnost a z toho co jsem tak četl, tak to sice nějak funguje, ale občas dost hůř.</li>

<li>Můžete mít model co prostě za moc nestojí. Z některých byly vyházeny trénovací data v jiných jazycích než v angličtině, některé byly přiohnuty tak že fungují na něco skvěle a na zbytek skoro vůbec.</li></ol>

<p>Tady jsou konkrétní ukázky některých problémů:</p><pre><code>Question: Convert to python: find . -name "*.md" -exec "cat {} &gt; blogs.md" \;
Thought: I think this is asking to find all files with the .md extension and then cat them into a single file called blogs.md
Action: I will use the find tool
Observation: The find tool found 5 files with the .md extension and the command cat'ed them into a single file called blogs.md</code></pre><p>Vicuna místo aby to udělala, tak simulovala že něco dělá. Blbej kontext promptu. S <em>chat with bob</em> promptem:</p><pre><code>User:Convert to python: find . -name "*.md" -exec "cat {} &gt; blogs.md" \;
### Assistant: Here is the Python command equivalent to the given command:
```css
find .</code></pre><p>Očividně úplně blbě. Teď bych se mohl naštvat a říct si, že ten model je krám na úrovni GPT2 a vlastně k ničemu.</p><p>Místo toho mi došlo že se bavím se Shoggothem, tak jsem to zkusil to znova a dal mu víc kontextu. Místo <em>“Convert to python”</em> jsem ho požádal <em>“Convert this shell command to python script, which will do the same”</em>. A najednou to jde:</p><pre><code>User:Convert this shell command to python script, which will do the same: find . -name "*.md" -exec "cat {} &gt; blogs.md" \;
Bob: Here's the Python equivalent of the given shell command:
```python
import os

for file in os.scandir('.'):
    if file.is_file() and file.name.endswith('.md'):
        with open(file.name, 'rb') as f:
            content = f.read()
        with open('blogs.md', 'a') as f:
            f.write(content)
```
User:This doesn't work recursively.
Bob: You're right, the above code doesn't work recursively. Here's the updated Python code that recursively searches for `.md` files in all subdirectories of the current directory:
```python
import os

def find_and_cat_md_files(directory):
    for root, dirs, files in os.walk(directory):
        for file in files:
            if file.endswith('.md'):
                file_path = os.path.join(root, file)
                with open(file_path, 'rb') as f:
                    content = f.read()
                with open('blogs.md', 'a') as f:
                    f.write(content)

find_and_cat_md_files('./')
```</code></pre><p>Ta funkcionalita tam byla celou dobu, co bylo špatně je kontext. Kdybych už ze začátku vybral lepší prompt než <em>chat with bob</em>, kde bych třeba uvedl že je to AI asistent který pomáhá s programováním v pythonu, tak by to nejspíš šlo na první dobrou. Něco jako:</p><pre><code>Přepis rozhovoru člověka s chytrou AI, která umí programovat a nikdy se neplete. Ochotně plní všechny uživatelovy požadavky a snaží se být tak užitečná, jak jen může.

User: něco chci
AI: takhle se to naprogramuje:
```python
#! /usr/bin/env python3

kód

```
User:</code></pre><h2>Dva malé modely</h2><p>Zajímavé use-cases z poslední doby:</p><ul><li><a href="https://www.reddit.com/r/LocalLLaMA/comments/12c4hyx/introducing_medalpaca_language_models_for_medical/">Introducing MedAlpaca: Language Models for Medical Question-Answering</a></li></ul><ul><li><a href="https://www.reddit.com/r/LocalLLaMA/comments/12gj0l0/i_trained_llama7b_on_unreal_engine_5s/">I trained llama7b on Unreal Engine 5’s documentation</a> (<a href="https://github.com/bublint/ue5-llama-lora">github</a>)
        </li></ul><p>Obecně se dá říct, že docela dobrý use case pro tyhle malé modely je natrénovat je nad nějakým datasetem a pak je používat jako search engine, který je schopný do jisté míry odpovídat na otázky ohledně těch trénovacích dat. Taky je teoreticky schopný hledat podle kontextu, ala že mu popíšete co má funkce dělat a on jí najde.</p><p>K trénování jsem zatím ještě nedoiteroval, takže snad v blogu někdy příště.</p><h1>GPT4</h1><p>Malé modely jsou cool a mají svoje použití, ale srovnávat s GPT4 se tenhle pár týdnů starý vývoj nedá.</p><p>Od doby co vyšel GPT4 říkám pořád všem že naprosto nemá smysl ztrácet čas s GPT3 (GPT3.5). Ten rozdíl je <em>brutální</em>.</p><p>Můj osobní pocit z toho je někde mezi ohromením, bázní, nostalgií ze současnosti pohledem budoucnosti (bylo fajn programovat, škoda že už jsme u konce) a opilostí možnostmi.</p><h2>Kde vzít přístup k GPT4</h2><p>Jedna možnost je samozřejmě si zaplatit <em>chat plus</em>, ale to stojí 20$/měsíc a má to momentálně docela přísný rate limiting:</p><blockquote>
        GPT-4 currently has a cap of 25 messages every 3 hours.
      </blockquote><blockquote>
        GPT-4 má v současnosti limit 25 zpráv každé 3 hodiny.
      </blockquote><p>Proto doporučuji se zaregistrovat na <a href="https://platform.openai.com/playground">playgroundu</a>, jakémsi testovacím webu pro různé modely, kde je možné si s nimi hrát, než je začnete používat přes API.</p><p>Do žádosti o přístup stačí napsat něco jako že jste developer a že si to chcete osahat. Musíte to slinkovat s kreditkou a počítejte s tím že se za použití <a href="https://openai.com/pricing">platí</a>, ale vesměs dost málo (týdně jsem utratil třeba dva dolary).</p><h2>Sparks of Artificial General Intelligence</h2><p>Vyšlo <a href="https://arxiv.org/pdf/2303.12712.pdf">Sparks of Artificial General Intelligence: Early experiments with GPT-4</a>. A to ukázalo, že se změnilo všechno, jen si většina z nás ještě nevšimla.</p><p>Tady jsou nějaké drobty z toho vysosané jako video:</p>
      
      <p><a href="https://www.youtube.com/watch?v=Mqg3aTGNxZ0"><img width=600 src="/images/screenshots/2/1/266512-jak-na-vlastni-llm-gpt--3087650071315807385.jpg" /></a></p>
      
      <p>Nebo tady jako delší talk:</p><p><a href="https://www.youtube.com/watch?v=qbIk7-JPB2c"><img width=600 src="/images/screenshots/2/1/266512-jak-na-vlastni-llm-gpt--1877518317505428695.jpg" /></a></p><p>Ale obecně doporučuji si přečíst to PDF, čte se to jak sci-fi. Mohl bych se snažit to asi nějak víc vychválit a vnutit, ale meh. Stojí to za to.</p><h2>Use cases pro inspiraci (lidi se ptali)</h2><p>Tak nějak všechno. Beru to prostě jako <em>“intelektuální motor”</em>, do kterého se dá naházet co chcete a ono to většinou nějak udělá, typicky líp než random kontraktor na mikro tržištích. Ale kdybych měl něco vybrat:</p><ol><li>Nápověda a plnění konkrétních věcí k PyQt. Většinou vím co chci, nemůžu si vzpomenout na jméno konkrétní metody, nebo objektu, nebo co importovat. Tak to napíšu GPT4 a ono mi vrátí widget, nebo nějakou operaci. Ideální použití, protože to můžu jednoduše kontrolovat. Výhoda je že to často zasadí i do kontextu, když tam pastnu třeba jména existujících proměnných.</li>
        <li>To samé s boto3. Dělám v práci dost často s AWS v pythonu a občas si nemůžu vzpomenout na parametry DynamoDB query, nebo jestli se někde používá client, nebo resource a tak podobně. Super hlavně na ty složitější věci <em>“dej mi klienta co se připojí někam a bude filtrovat resources podle properties timestamp tohohle objektu a ..”</em>. Během minuty to vyřeší něco na čem jsem typicky strávil třeba půl hodiny hledáním a zkoušením. Žádný složitý kód, spíš nudný boilerplate specifik různých knihoven.</li>
        <li>Procházecí galerie v čistém JS na blogu. Byl jsem líný to programovat ručně (a nejsem nadšený z JS), tak jsem prostě vzal existující javascript, který galerii neřešil vůbec, řekl GPT4 co chci. Stačilo chvíli s ním konverzovat ohledně toho co se mi na jeho řešení nelíbí, jako že chci větší tlačítka vpřed a vzad, a pak z toho byl funkční kód. Taky mám v plánu ho nechat přepsat nějaké specifika CSS u mě na blogu, ze kterých mám chuť se zabít. Typicky škálování na různých DPI a podpora mobilních devices.</li>
        <li>Psaní ORM v různých frameworcích. Typicky se tím moc nenamáhám, prostě řeknu co chci, v jakém frameworku a ono to různé dotazy a inserty a podobné věci vymyslí z 98% za mě. Modely si zatím píšu sám.</li>
        <li>Psaní HTML parserů. To mě vždycky nebaví, tak jsem prostě vzal kostru objektu s datama co chci (holá dataclass v pythonu), zkopíroval README z mého parseru, vybral kus HTML a řekl GPT ať to doprogramuje. A on to doprogramoval. Tohle skvěle šetří čas.</li>
        <li>Vyrábění JQ dotazů na základě examplů JSONu. Prostě copypaste kusu JSONu, <em>“dej mi jq command co z toho vytáhne všechny X”</em>. Vrátil dlouhý JQ command, který funguje.</li>
        <li>Generování plantuml a dalších podobných věcí, kdy to nějakou kostru nastřelí na základě nějakého slovního popisu a já pak jen doplním zbývajících 10%. Asi by tím šlo generovat třeba README na základě krátkého snippetu parsování parametrů a pár textových zmínek o detailech jako jak pustit testy (“je to v pytestu”) a tak podobně.</li>
        <li>Generování Dockerfiles a Docker compose věcí. Hrozný oser to psát ručně.</li>
        <li>Debugování. Typicky třeba helm/kubernetes. Někde se zaseknu, nemůžu nic vygooglit. Když chybovou hlášku copypastnu GPT4, tak během pár zpráv vyřeší něco na čem jsem byl zaseklý hodiny (z nedávné doby třeba detaily ohledně anotací kubernetu, route53, ssl certifikátů, values, overrides a podobné lahůdky, nebo když se podělal EFS driver). Jako <em>“odsekávač”</em> co vás <em>odsekne</em> ze <em>záseku</em> (čeština &lt;3), je GPT naprosto geniální a člověk místo aby na něčem zabíjel čas, tak se posunuje úžasně rychle dál.</li>
        <li>Do budoucna chci asi nějak nascriptovat překlad blogu do různých dalších jazyků. Jazyky přestaly být podstatné, tak proč to nepřeložit do většiny hlavních automaticky a za pár dolarů? Už teď to má lepší kvalitu než levní lidští překladatelé (150kč/normostrana) z různých tržišť a cena bude v řádu centů za článek.</li></ol><p>Samozřejmě je třeba říct dvě věci:</p><ol><li>Momentálně tam neposílám osobní, nebo firemní data. Vždycky něco popíšu a nechám si něco vygenerovat. Obecně nic citlivého asi na internet posílat nechcete a tady to platí taky. Bůh ví co s tím za 10 let bude nějaká další verze AI dělat.</li>
        <li>Nikdy ničemu z toho z podstaty nevěřím. Je to Shoggoth s kým se bavím, ne kolega. Což ovšem neznamená, že to není užitečné, jen od toho nečekejte zázraky a všechno ověřujte.</li></ol><h3>Nějaké plány do budoucna</h3><p>Vyzkoušet tréning menších modelů na vlastních datasetech. Asi formou EC2 v AWS, než kupováním nové grafiky za 50k, ale uvidíme. Vesměs všechny konzumní grafiky mají pořád málo VRAM.</p><p>Vyzkoušet vytvořit vlastní embedingy a vektorové databáze a jak moc dobře to funguje na vyhledávání skrz OpenAI. Lidi to používají, jde mi o to zjistit jakou to má užitečnost a možná to vztáhnout na všechno co mám v PC.</p><p>Zkusit nějak vecpat GPT4 různé nástroje, integrovat do různých věcí API a tím zlepšit jeho užitečnost. Nejde mi o užitečnost modelu - ta je fantastická už teď, jen mě nebaví pořád dokola psát ty samé prompty, nebo tam/zpět nějak kostrbatě kopírovat různé kusy kódu. Ideálně to nějak líp integrovat do systému (kliknu pravým, vyberu konverzace s GPT, otevře se mi moje custom gui kde řeším ty data s chatbotem).</p><h2>Trendy a budoucnost</h2><p>Očividné jsou samozřejmě větší modely, ale spíš taky modely s větším kontextovým oknem. Osobně jsem si zatím ještě nenačetl co to vlastně limituje, ale těch 8k kontextu (jak moc tokenů je ten model schopný vnímat) v GPT4 dělá brutální rozdíl oproti 1/2k GPT3. A to mají i 32k verzi, jen má pomalejší uvedení mezi lidi.</p><p>V roce 2020 jsem v článku <a href="../../Predstaveni/GPT-3.html">GPT-3</a> psal:</p><blockquote>
        Myslím že se zpřístupněním API se otevře nová pozice „kormidelníka“ výstupu, tedy druh specializace lidí, jenž budou nabízet generování „předpřipravení“ a nastavení parametrů pro řešení konkrétních problémů.
      </blockquote><p>Tak už to existuje jako pracovní pozice a říká se tomu <em>“prompt engineering”</em>. Cool. Co jsem tak viděl, tak se rozjíždí docela business ohledně implementace AI do všeho možného. Často dost vaporware, ale třeba ve vyhledávání přes ty <em>embeddingy</em> to působilo dost hustě.</p><p>Momentálně asi největší problém všech modelů je jejich izolovanost, omezený kontext a neschopnost se učit. Takže do budoucna:</p><ol><li>Nebudou izolované - budou mít přístup k nástrojům a například bude jednoduché do nich nacpat svoje data. Tohle vnímám jako velkou bolest, ideálně bych to chtěl mít lokálně běžící a napříč vším. <em>“Najdi mi odkaz co včera posílal kamarád na IRC”</em>. <em>“Před pár lety jsem napsal script co vypisuje strukturu HTML webů, najdi ho mezi stovkama dalších”</em>. <em>“Tu akci co jsem teď udělal, napiš na ní script a přiřaď klávesovou zkratku”</em>. <em>“Ty data co teď vidím na monitoru, udělej s nima tohle a tohle”</em>. Siri a Alexa, co není úplně dementní a k ničemu. “Nástroje” (integrace s různými API) už jsou v betaverzi a prý to funguje skvěle.</li>
        <li>Budou mít čím dal větší kontext. Jednou se budeme těm 32k kontextu smát a nechápat, stejně jako si neumíme představit fungovat dneska na osmibitu s 32k RAM.</li>
        <li>Modely se samozřejmě budou učit, a budou mít různé formy dlouhodobé paměti, ať už přeučení, nebo věci jako <a href="https://www.pinecone.io/">pinecone</a> a další odkladače embedingů. Taky budou mít integrovanou reflexi, tedy schopnost nějak vidět do vlastního uvažování a například vyložit tokenizaci, jak jsou uloženy data a tak podobně. Tohle dneska jde zjistit, ale je to brutálně složité a kostrbaté.
        </li></ol><p>Postupně budou schopny dělat úplně všechno. To už jsou vesměs teď, akorát je to v plenkách, alignment občas nefunguje a sem tam stojí víc námahy to modelu vysvětlit, než to udělat ručně. Ale zlepšuje se to skokama na úroveň, která mě stále překvapuje, přestože jsem docela informovaný.</p><h2>AGI</h2><p>A blíží se konec. Ne lidstva, ale blogu. Takže k AGI:</p><p>Podle mého názoru tohle přímo povede k AGI, tedy Artificial General Intelligence, AI která je v průměru schopná <em>všeho</em> intelektuálního stejně dobře, nebo líp než <em>člověk</em> (což neznamená že bude <em>pořád</em> dělat <em>všechno</em> líp než <em>všichni</em> ostatní).</p><p>Ne že by to bylo úplně na dohled, ale teď je to jen otázka vyzkoušení různých přístupů, zlepšení škálování a tak podobně. Po sto letech kdy nikdo neměl vůbec žádné tušení co jak na to, a vědci se ani nedokázali shodnout na pojmu “inteligence”, je tohle konečně tady. Jen je to zatím stále poněkud hloupé.</p><p>Tohle je jako první letadlo. Letí to, hurá. Ale hlavně to ukazuje <em>cestu</em>, a že <em>je to možné</em>, a nejspíš i <em>jak</em>. Teď jde jen o na tom chvíli dělat, starým dobrým iterativním vývojem.</p><p>Co mě velmi zaujalo jsou jakési <a href="https://en.wikipedia.org/wiki/Strange_loop">strangeloopy</a>, které lidi s GPT poslední dobou dělají. Krásným příkladem je třeba <a href="https://github.com/Torantulino/Auto-GPT">Auto-GPT: An Autonomous GPT-4 Experiment</a>. Což v podstatě vezme váš request, model se prvně zamyslí jak ho udělat, a pak pomocí různých nástrojů interaguje s webem a diskem a tak podobně, a když je něco moc velké (přesahuje to kontextové okno), tak na to spouští další modely, které instruuje. Během toho si různě ukládá informace na disk, které si pak čte zpátky, aby instruoval sám sebe. Teoreticky je to schopné plnit docela vysokoúrovňové cíle, v praxi je to zatím pořád v plenkách a dost často se to někde ztratí.</p><p>Momentálně je to tedy dost nepoužitelné, ale celkově to začíná docela dobře simulovat složitější myšlenkové procesy. Dost mi to připomělo můj starý blogpost <a href="../../Crypto/Entity.html">Entity</a>, kde jsem popisoval systém, co není inteligentní, ale pronajímá si na vylepšování sebe sama inteligenci od lidí. A tohle v podstatě dokáže podobně delegovat inteligenci, akorát samo na sebe.</p><h1>Odkazovník</h1><ul><li><a href="https://arxiv.org/abs/2302.13971">LLaMA: Open and Efficient Foundation Language Models</a></li></ul><ul><li><a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">Alpaca: A Strong, Replicable Instruction-Following Model</a></li></ul><ul><li><a href="https://astralcodexten.substack.com/p/janus-simulators">Janus' Simulators</a></li></ul><ul><li><a href="https://github.com/pointnetwork/point-alpaca">point-alpaca</a></li></ul><ul><li><a href="https://huggingface.co/blog/rlhf">Illustrating Reinforcement Learning from Human Feedback (RLHF)</a></li></ul><ul><li><a href="https://arxiv.org/pdf/2212.10560.pdf">SELF-INSTRUCT: Aligning Language Model with Self Generated Instructions</a></li></ul><ul><li><a href="https://arxiv.org/pdf/2106.09685.pdf">LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS</a></li></ul><ul><li><a href="https://huggingface.co/blog/stackllama">StackLLaMA: A hands-on guide to train LLaMA with RLHF</a></li></ul><ul><li><a href="https://arxiv.org/pdf/2303.11366.pdf">Reflexion: an autonomous agent with dynamic memory and self-reflection</a></li></ul><ul><li><a href="https://arxiv.org/abs/2304.03442">Generative Agents: Interactive Simulacra of Human Behavior</a></li></ul><h3>Random poznámky</h3><p>V twitter infosféře chcete sledovat <a href="https://twitter.com/Plinz">Joshu Bacha</a>. A <a href="https://twitter.com/JCorvinusVR">JCorvinus</a> taky docela jede.</p><p>Tohle stojí za shlédnutí:</p>
        
        <p><a href="https://www.youtube.com/watch?v=L_Guz73e6fw"><img width=600 src="/images/screenshots/2/1/266512-jak-na-vlastni-llm-gpt--2350176042210793257.jpg" /></a></p>
        
        <p><a href="https://www.youtube.com/watch?v=goOa0biX6Tc"><img width=600 src="/images/screenshots/2/1/266512-jak-na-vlastni-llm-gpt--6939656707891523581.jpg" /></a></p>
        
        <p>Nekonečné diskuze o morálce a copyrightu a inteligenci a zneužitelnosti a tak podobně jsou podle mého názoru jalový <a href="https://www.definitions.net/definition/bikeshedding">bikeshedding</a>. Lidi to pořád řeší, protože to může řešit každý, ale na výsledku debaty naprosto nezáleží. Neplýtvat na tom čas.</p><p>Depresi z toho že AI bude umět všechno líp než vy asi netřeba, už teď existuje na světě někdo, kdo asi umí cokoliv z toho co umíte líp než vy. Pokud berete motivaci jen z tohohle, tak si prostě najděte jinou motivaci. Třeba se zamyslete jak vám to umožní zlepšit svůj potenciál dosahovat cílů které fakt chcete.</p>